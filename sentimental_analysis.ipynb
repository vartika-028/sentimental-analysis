{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bv3inm7BCGqR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.DataFrame({\n",
        "    'sentiment': ['Positive', 'Negative', 'Positive', 'Negative', 'Positive', 'Negative',\n",
        "                  'Positive', 'Negative', 'Positive', 'Negative'],\n",
        "    'review': [\n",
        "        'I love this product, it is amazing and works great!',\n",
        "        'I hate this product, it is terrible and a waste of money.',\n",
        "        'Absolutely fantastic experience, would buy again!',\n",
        "        'Worst experience ever. Totally disappointed.',\n",
        "        'Great value for money, exceeded my expectations.',\n",
        "        'Horrible quality. Completely useless.',\n",
        "        'This is awesome and super useful.',\n",
        "        'This is awful and broke in one day.',\n",
        "        'I am happy with this purchase.',\n",
        "        'I regret buying this gadget.'\n",
        "    ]\n",
        "})\n"
      ],
      "metadata": {
        "id": "ux9u4XxCCIPj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data['review']\n",
        "y = data['sentiment']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n",
        "\n",
        "preprocessor = lambda text: re.sub(r'[^a-z ]', '', text.lower())"
      ],
      "metadata": {
        "id": "nNSSiP-ACcVm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pipe = Pipeline([\n",
        "    ('vec', CountVectorizer(stop_words='english', preprocessor=preprocessor)),\n",
        "    ('tfid', TfidfTransformer()),\n",
        "    ('clf', SGDClassifier(loss='log_loss', random_state=0))\n",
        "])\n",
        "\n",
        "model = pipe.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "99AfXMtOCfss"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y_test_pred = model.predict(X_test)\n",
        "print(\"\\nModel Evaluation on Test Data:\\n\")\n",
        "print(classification_report(y_test, y_test_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxHC_-HyCh1l",
        "outputId": "871065cc-2ff4-455d-e162-627917b0078f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Evaluation on Test Data:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.00      0.00      0.00       0.0\n",
            "    Positive       0.00      0.00      0.00       2.0\n",
            "\n",
            "    accuracy                           0.00       2.0\n",
            "   macro avg       0.00      0.00      0.00       2.0\n",
            "weighted avg       0.00      0.00      0.00       2.0\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_cases = {\n",
        "    'This gadget is awesome': 'Positive',\n",
        "    'This gadget is terrible': 'Negative',\n",
        "\n",
        "}\n",
        "\n",
        "predictions = []\n",
        "for text, expected in test_cases.items():\n",
        "    predicted = model.predict([text])[0]\n",
        "    predictions.append([text, expected, predicted])\n",
        "\n",
        "df = pd.DataFrame(predictions, columns=['Test Case', 'Expected', 'Predicted'])\n",
        "print(\"\\nCustom Predictions:\\n\")\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLalXiHPCj1-",
        "outputId": "875a445e-ce7b-4b29-949b-cde1b910aad6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Custom Predictions:\n",
            "\n",
            "                 Test Case  Expected Predicted\n",
            "0   This gadget is awesome  Positive  Positive\n",
            "1  This gadget is terrible  Negative  Negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l281yEX2CoUW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}